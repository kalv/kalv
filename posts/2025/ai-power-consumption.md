---
title: AI Power consumption
description: AI Power consumption
date: 2025-11-13
layout: layouts/post.njk
tags: ["computers"]
---

Today it's interesting that AI is everywhere but in the cloud and not local private indexes utilizing the CPUs or neural cores that are available. It's interesting that no research and development is being continued from techniques in the early 2000s to use search indexes and NLP knowledge extraction. To create semantic structures and intelligent search for questions.

Most of the time, I use Gemini, OpenAI and Duck.ai to ask questions that are literally just searches on knowledge that is available and index-able on the internet but structured such that one question gets asked and subsequent questions get asked to compose the answer. The answer is then summarized using NLP and other text summarization such that you can easily read the answer. The same is done with code generation. This could easily be done locally and privately ensuring that we don't use these Large one model does all function.

The downside of the current approach with AI is power consumption. These large GPUs to compute these large models of data, even to answer a question or even build these indexes for version updates are insane.

Based on current estimates, the total energy consumed by AI hardware in data centers (primarily cloud GPUs) is likely in the range of 55 to 110 Terawatt-hours (TWh) per year.

To put this into perspective, the total power consumption of Switzerland is 60 TWh per year, Israel, 57 TWh per year and then the top end is Netherlands with 115 TWh per year.

These AI question answer services must be worth it, but really what is this going to do for the energy costs globally and perhaps shifts of ownership on what makes electricity. Insane.
